{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO8noPwhf2z30qvPjrK9KLj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mYTFbXoziwfc","executionInfo":{"status":"ok","timestamp":1744625514297,"user_tz":-210,"elapsed":4556,"user":{"displayName":"فاطمه قربانی","userId":"17913424939683341749"}},"outputId":"471d405d-672c-4cb2-d513-84a2f6da5b10"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n"]},{"cell_type":"markdown","source":["To ensure robust model performance, all datasets were divided into three subsets: 70% for training, 20% for validation, and 10% for testing. A stratified splitting strategy was employed to maintain balanced class distributions of AMPs and non-AMPs across all subsets.(DS1)"],"metadata":{"id":"06zpU0bN_LsA"}},{"cell_type":"code","source":["import torch\n","from transformers import BertModel, BertTokenizer, XLNetModel, XLNetTokenizer, T5EncoderModel, T5Tokenizer\n","import re\n","import os\n","import requests\n","import gc\n","from tqdm.auto import tqdm\n","import pandas as pd\n","import numpy as np"],"metadata":{"id":"CSEXh66DnV4c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.utils import shuffle\n","from sklearn.model_selection import train_test_split\n","\n","# خواندن داده‌ها و برچسب‌ها از فایل‌ها\n","X_train = pd.read_csv('/content/drive/MyDrive/my_project/LMPred_Embeddings/X_train.csv')\n","X_val = pd.read_csv('/content/drive/MyDrive/my_project/LMPred_Embeddings/X_val.csv')\n","X_test = pd.read_csv('/content/drive/MyDrive/my_project/LMPred_Embeddings/X_test.csv')\n","\n","y_train = pd.read_csv('/content/drive/MyDrive/my_project/LMPred_Embeddings/y_train.csv', header=None).values\n","y_val = pd.read_csv('/content/drive/MyDrive/my_project/LMPred_Embeddings/y_val.csv', header=None).values\n","y_test = pd.read_csv('/content/drive/MyDrive/my_project/LMPred_Embeddings/y_test.csv', header=None).values\n","\n","# ترکیب داده‌ها و برچسب‌ها\n","X_total = pd.concat([X_train, X_val, X_test], ignore_index=True)\n","y_total = np.concatenate([y_train, y_val, y_test], axis=0)\n","\n","# شافل کردن داده‌ها و برچسب‌ها به طور همزمان\n","X_total, y_total = shuffle(X_total, y_total, random_state=42)\n","\n","# تقسیم داده‌ها به نسبت‌های مشخص شده\n","X_train_final, X_temp, y_train_final, y_temp = train_test_split(X_total, y_total, test_size=0.3, random_state=42)\n","X_val_final, X_test_final, y_val_final, y_test_final = train_test_split(X_temp, y_temp, test_size=1/3, random_state=42)  # 1/3 از 0.3 می‌شود 0.1\n","\n","# ذخیره داده‌ها و برچسب‌ها در فایل‌های جدید\n","X_train_final.to_csv('/content/drive/MyDrive/my_project/LMPred_Embeddings/X_train_final.csv', index=False)\n","pd.DataFrame(y_train_final).to_csv('/content/drive/MyDrive/my_project/LMPred_Embeddings/y_train_final.csv', index=False, header=False)\n","\n","X_val_final.to_csv('/content/drive/MyDrive/my_project/LMPred_Embeddings/X_val_final.csv', index=False)\n","pd.DataFrame(y_val_final).to_csv('/content/drive/MyDrive/my_project/LMPred_Embeddings/y_val_final.csv', index=False, header=False)\n","\n","X_test_final.to_csv('/content/drive/MyDrive/my_project/LMPred_Embeddings/X_test_final.csv', index=False)\n","pd.DataFrame(y_test_final).to_csv('/content/drive/MyDrive/my_project/LMPred_Embeddings/y_test_final.csv', index=False, header=False)\n","\n","# بررسی ابعاد فایل‌ها\n","print(f\"X_train_final: {X_train_final.shape}\")\n","print(f\"y_train_final: {y_train_final.shape}\")\n","print(f\"X_val_final: {X_val_final.shape}\")\n","print(f\"y_val_final: {y_val_final.shape}\")\n","print(f\"X_test_final: {X_test_final.shape}\")\n","print(f\"y_test_final: {y_test_final.shape}\")\n"],"metadata":{"id":"oMhT3YFPnV7r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Feature Vector Extraction,T5-XL,LMPred_Embeddings(DS1)"],"metadata":{"id":"AE2xGz-uqklU"}},{"cell_type":"code","source":["!pip install -q transformers\n","!pip install -q transformers sentencePiece"],"metadata":{"id":"nQSjm8TKnSYW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from transformers import BertModel, BertTokenizer, XLNetModel, XLNetTokenizer, T5EncoderModel, T5Tokenizer\n","import re\n","import os\n","import requests\n","import gc\n","from tqdm.auto import tqdm\n","import pandas as pd\n","import numpy as np"],"metadata":{"id":"Rz4yFI9BnSbj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","# اتصال Google Drive به محیط کاری\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# مسیر پوشه جدید در Google Drive\n","folder_path = '/content/drive/MyDrive/my_project/LMPred_Embeddings'\n","\n","# بررسی وجود پوشه و ایجاد آن در صورت عدم وجود\n","if not os.path.exists(folder_path):\n","    os.makedirs(folder_path)\n","    print(f\"Folder '{folder_path}' created successfully.\")\n","else:\n","    print(f\"Folder '{folder_path}' already exists.\")\n"],"metadata":{"id":"Ra6Cc8BsnSeS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import numpy as np\n","import torch\n","import re\n","from transformers import BertTokenizer, BertModel, T5Tokenizer, T5EncoderModel, XLNetTokenizer, XLNetModel\n","\n","class LM_EMBED:\n","    def __init__(self, language_model, max_len, rare_aa):\n","        self.lang_model = language_model\n","        self.max_len = max_len\n","        self.rare_aa = rare_aa\n","\n","        # وارد کردن توکنایزر و مدل از Rostlab ProtTrans:\n","        self.tokenizer, self.model = self._load_model_and_tokenizer()\n","\n","        # تنظیم دستگاه به GPU در صورت موجود بودن:\n","        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","        self.model = self.model.to(self.device).eval()\n","\n","    def _load_model_and_tokenizer(self):\n","        model_map = {\n","            'BERT-BFD': ('Rostlab/prot_bert_bfd', BertTokenizer, BertModel),\n","            'BERT': ('Rostlab/prot_bert', BertTokenizer, BertModel),\n","            'T5-XL-BFD': ('Rostlab/prot_t5_xl_bfd', T5Tokenizer, T5EncoderModel),\n","            'T5-XL-UNI': ('Rostlab/prot_t5_xl_uniref50', T5Tokenizer, T5EncoderModel),\n","            'XLNET': ('Rostlab/prot_xlnet', XLNetTokenizer, XLNetModel)\n","        }\n","\n","        tokenizer_name, tokenizer_class, model_class = model_map.get(self.lang_model, (None, None, None))\n","        if tokenizer_name is None or tokenizer_class is None or model_class is None:\n","            raise ValueError(\"مدل زبانی پشتیبانی نمی‌شود\")\n","\n","        tokenizer = tokenizer_class.from_pretrained(tokenizer_name, do_lower_case=False)\n","        model = model_class.from_pretrained(tokenizer_name)\n","\n","        # برای XLNet، تنظیم mem_len\n","        if self.lang_model == 'XLNET':\n","            model.config.mem_len = 512\n","\n","        return tokenizer, model\n","\n","    def add_spaces(self, seqs_list):\n","        return [\" \".join(sequence) for sequence in seqs_list]\n","\n","    def extract_word_embs(self, seq_df, filename):\n","        # ساختن لیستی از توالی‌ها از دیتافریم:\n","        seqs_list = seq_df.Sequence.to_list()\n","        seqs_spaced = self.add_spaces(seqs_list)\n","\n","        if self.rare_aa:\n","            seqs_spaced = [re.sub(r\"[UZOB]\", \"X\", sequence) for sequence in seqs_spaced]\n","\n","        ids = self.tokenizer.batch_encode_plus(seqs_spaced, add_special_tokens=True, padding='max_length', max_length=self.max_len)\n","        input_ids = torch.tensor(ids['input_ids']).to(self.device)\n","        attention_mask = torch.tensor(ids['attention_mask']).to(self.device)\n","\n","        # پردازش توالی‌ها به صورت دسته‌ای\n","        batch_size = 10\n","        num_batches = (len(input_ids) + batch_size - 1) // batch_size\n","        all_embeddings = []\n","\n","        with torch.no_grad():\n","            for i in range(num_batches):\n","                start = i * batch_size\n","                end = min((i + 1) * batch_size, len(input_ids))\n","                batch_input_ids = input_ids[start:end]\n","                batch_attention_mask = attention_mask[start:end]\n","\n","                embeddings = self.model(input_ids=batch_input_ids, attention_mask=batch_attention_mask)[0]\n","                all_embeddings.append(embeddings.cpu().numpy())\n","\n","        embedding_res = np.concatenate(all_embeddings, axis=0)\n","        features = self.extract_features(embedding_res, attention_mask.cpu().numpy())\n","        padded_arr = self.pad(features)\n","\n","        # مسیر ذخیره‌سازی امبدینگ\n","        embedding_folder = \"/content/drive/MyDrive/my_project/LMPred_Embeddings\"\n","        os.makedirs(embedding_folder, exist_ok=True)\n","        full_path = os.path.join(embedding_folder, filename)\n","\n","        # ذخیره آرایه:\n","        print(\"در حال ذخیره امبدینگ‌ها...\")\n","        np.save(full_path, padded_arr)\n","\n","    def extract_features(self, emb_res, att_msk):\n","        features = []\n","        for seq_num in range(len(emb_res)):\n","            seq_len = (att_msk[seq_num] == 1).sum()\n","            if self.lang_model in ['BERT-BFD', 'BERT']:\n","                seq_emd = emb_res[seq_num][1:seq_len-1]\n","            elif self.lang_model in ['T5-XL-BFD', 'T5-XL-UNI']:\n","                seq_emd = emb_res[seq_num][:seq_len-1]\n","            elif self.lang_model == 'XLNET':\n","                padded_seq_len = len(att_msk[seq_num])\n","                seq_emd = emb_res[seq_num][padded_seq_len-seq_len:padded_seq_len-2]\n","            features.append(seq_emd)\n","\n","        return np.array(features, dtype=object)\n","\n","    def pad(self, features):\n","        dim1 = self.max_len - 2  # کاهش دو واحد برای توکن‌های CLS و SEP که از قبل حذف شده‌اند\n","        dim2 = features[0].shape[1]\n","        padded_arr = np.zeros((len(features), dim1, dim2))\n","\n","        for i, feature in enumerate(features):\n","            padded_arr[i, :feature.shape[0], :feature.shape[1]] = feature\n","\n","        return padded_arr\n"],"metadata":{"id":"VQtIqlrKnShW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_val = pd.read_csv('/content/drive/MyDrive/my_project/LMPred_AMP_Prediction/LM_Pred_Dataset/X_val_final.csv')\n"],"metadata":{"id":"a_rY4lXRnSj_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_test = pd.read_csv('/content/drive/MyDrive/my_project/LMPred_AMP_Prediction/X_test_final.csv')\n"],"metadata":{"id":"kecOhNqTnSm4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train = pd.read_csv('/content/drive/MyDrive/my_project/LMPred_AMP_Prediction/X_train_final.csv')\n"],"metadata":{"id":"pOL0E7aBnSpt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Specifying the max sequence length in the given dataset (255 for the LMPred Dataset),\n","# then adding 2 to account for special [CLS, SEP] tokens added by the language models):\n","max_seq_len = 257\n","T5XL_UNI_EMBED = LM_EMBED('T5-XL-UNI', max_seq_len, True)"],"metadata":{"id":"P181xIZPnSsD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","# بارگذاری داده‌ها از CSV\n","X_train = pd.read_csv('/content/drive/MyDrive/my_project/LMPred_AMP_Prediction/LM_Pred_Dataset/X_train_final.csv')\n","\n","# پارامترهای مربوط به دسته‌های داده\n","batch_size = 1000  # تعداد نمونه‌ها در هر دسته\n","num_batches = (len(X_train) + batch_size - 1) // batch_size  # تعداد کل دسته‌ها\n","\n","# مسیر خروجی برای ذخیره امبدینگ‌ها\n","output_dir = '/content/drive/MyDrive/my_project/LMPred_Embeddings/'\n","\n","# اطمینان از اینکه آبجکت T5XL_UNI_EMBED به درستی ایجاد شده است\n","if 'T5XL_UNI_EMBED' not in locals():\n","    raise ValueError(\"آبجکت T5XL_UNI_EMBED تعریف نشده است.\")\n","\n","# پردازش دسته به دسته\n","for i in range(num_batches):\n","    # تعریف محدوده داده‌های فعلی\n","    start_index = i * batch_size\n","    end_index = min(start_index + batch_size, len(X_train))\n","\n","    # استخراج داده‌های فعلی\n","    X_train_batch = X_train.iloc[start_index:end_index]\n","\n","    # مسیر خروجی برای ذخیره امبدینگ‌های دسته فعلی\n","    output_batch = f'{output_dir}T5XL_UNI_INDEP_X_TRAIN_batch_{i+1}.npy'\n","\n","    # استخراج و ذخیره امبدینگ‌های دسته فعلی\n","    T5XL_UNI_EMBED.extract_word_embs(X_train_batch, output_batch)\n","\n","    print(f\"امبدینگ‌های دسته {i+1} با موفقیت استخراج و ذخیره شدند.\")\n","\n","print(\"تمام امبدینگ‌ها با موفقیت استخراج و ذخیره شدند.\")\n"],"metadata":{"id":"J1bmBwU9n0kK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# تعریف مسیر خروجی برای ذخیره امبدینگ‌های X_val\n","\n","output_val = '/content/drive/MyDrive/my_project/LMPred_Embeddings/T5XL_UNI_INDEP_X_val.npy'\n","\n","# استخراج و ذخیره امبدینگ‌های X_val\n","T5XL_UNI_EMBED.extract_word_embs(X_val, output_val)\n","\n","print(\"امبدینگ‌های X_val با موفقیت استخراج و ذخیره شدند.\")\n","\n","\n","# استخراج و ذخیره امبدینگ‌های X_val\n","T5XL_UNI_EMBED.extract_word_embs(X_val, output_val)\n","\n","print(\"امبدینگ‌های X_val با موفقیت استخراج و ذخیره شدند.\")\n"],"metadata":{"id":"-O3EhOwYoAAr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# تعریف مسیر خروجی برای ذخیره امبدینگ‌های X_test\n","\n","output_test = '/content/drive/MyDrive/my_project/LMPred_Embeddings/T5XL_UNI_INDEP_X_TEST.npy'\n","\n","# استخراج و ذخیره امبدینگ‌های X_test\n","T5XL_UNI_EMBED.extract_word_embs(X_test, output_test)\n","\n","print(\"امبدینگ‌های X_test با موفقیت استخراج و ذخیره شدند.\")\n","\n","\n","# استخراج و ذخیره امبدینگ‌های X_test\n","T5XL_UNI_EMBED.extract_word_embs(X_test, output_test)\n","\n","print(\"امبدینگ‌های X_test با موفقیت استخراج و ذخیره شدند.\")\n"],"metadata":{"id":"HGrbQdQ2n607"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["resized_X_val.npy"],"metadata":{"id":"h9bMbILvsQ8U"}},{"cell_type":"code","source":["import numpy as np\n","\n","def resize_and_save_data(file_paths, save_paths, target_shape):\n","    for file_path, save_path in zip(file_paths, save_paths):\n","        data = np.load(file_path)\n","        num_samples = data.shape[0]\n","        resized_data = np.zeros((num_samples, target_shape[0], target_shape[1]), dtype=np.float32)\n","        for i in range(num_samples):\n","            sample = data[i]\n","            # فرض بر این است که هر نمونه ممکن است ابعاد متغیری داشته باشد و باید برش یا پر شود\n","            resized_data[i, :sample.shape[0], :sample.shape[1]] = sample\n","        np.save(save_path, resized_data)\n","        print(f\"Saved resized data to {save_path}\")\n","\n","# مسیر فایل‌های اصلی و فایل‌های ذخیره شده برای داده‌های آموزشی\n","file_paths_X = [\n","    '/content/drive/MyDrive/my_project/LMPred_Embeddings/T5XL_UNI_INDEP_X_TRAIN_batch_1.npy',\n","    '/content/drive/MyDrive/my_project/LMPred_Embeddings/T5XL_UNI_INDEP_X_TRAIN_batch_2.npy',\n","    '/content/drive/MyDrive/my_project/LMPred_Embeddings/T5XL_UNI_INDEP_X_TRAIN_batch_3.npy',\n","    '/content/drive/MyDrive/my_project/LMPred_Embeddings/T5XL_UNI_INDEP_X_TRAIN_batch_4.npy',\n","    '/content/drive/MyDrive/my_project/LMPred_Embeddings/T5XL_UNI_INDEP_X_TRAIN_batch_5.npy',\n","    '/content/drive/MyDrive/my_project/LMPred_Embeddings/T5XL_UNI_INDEP_X_TRAIN_batch_6.npy'\n","]\n","save_paths_X = [\n","    '/content/drive/MyDrive/my_project/LMPred_Embeddings/resized_X_TRAIN_batch_1.npy',\n","    '/content/drive/MyDrive/my_project/LMPred_Embeddings/resized_X_TRAIN_batch_2.npy',\n","    '/content/drive/MyDrive/my_project/LMPred_Embeddings/resized_X_TRAIN_batch_3.npy',\n","    '/content/drive/MyDrive/my_project/LMPred_Embeddings/resized_X_TRAIN_batch_4.npy',\n","    '/content/drive/MyDrive/my_project/LMPred_Embeddings/resized_X_TRAIN_batch_5.npy',\n","    '/content/drive/MyDrive/my_project/LMPred_Embeddings/resized_X_TRAIN_batch_6.npy'\n","]\n","\n","# مسیر فایل‌های اعتبارسنجی\n","val_file_path_X = ['/content/drive/MyDrive/my_project/LMPred_Embeddings/T5XL_UNI_INDEP_X_val.npy']\n","val_save_path_X = ['/content/drive/MyDrive/my_project/LMPred_Embeddings/resized_X_val.npy']\n","\n","# تغییر اندازه و ذخیره داده‌های آموزشی\n","resize_and_save_data(file_paths_X, save_paths_X, target_shape=(255, 1024))\n","\n","# تغییر اندازه و ذخیره داده‌های اعتبارسنجی\n","resize_and_save_data(val_file_path_X, val_save_path_X, target_shape=(255, 1024))\n"],"metadata":{"id":"806Cdtdrn64D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["resized_X_TEST.npy"],"metadata":{"id":"VlFT9zS7sUzs"}},{"cell_type":"code","source":["import numpy as np\n","\n","def resize_and_save_data(file_path, save_path, target_shape):\n","    data = np.load(file_path)\n","    num_samples = data.shape[0]\n","    resized_data = np.zeros((num_samples, target_shape[0], target_shape[1]), dtype=np.float32)\n","    for i in range(num_samples):\n","        sample = data[i]\n","        # فرض بر این است که هر نمونه ممکن است ابعاد متغیری داشته باشد و باید برش یا پر شود\n","        resized_data[i, :sample.shape[0], :sample.shape[1]] = sample\n","    np.save(save_path, resized_data)\n","    print(f\"Saved resized data to {save_path}\")\n","\n","# مسیر فایل تست\n","test_file_path_X = '/content/drive/MyDrive/my_project/LMPred_Embeddings/T5XL_UNI_INDEP_X_TEST.npy'\n","\n","# مسیر ذخیره فایل تست\n","test_save_path_X = '/content/drive/MyDrive/my_project/LMPred_Embeddings/resized_X_TEST.npy'\n","\n","# تغییر اندازه و ذخیره داده تست\n","resize_and_save_data(test_file_path_X, test_save_path_X, target_shape=(255, 1024))\n"],"metadata":{"id":"HScGWu2yn66y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This part of the code is related to training and testing the final model, TrancBiM. The model first includes BiLSTM layers, followed by Transformer and attention mechanisms. These are my final results on the first dataset.\"\n"],"metadata":{"id":"69Bj6tuiyW38"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# تعریف دیتاست سفارشی\n","class CustomDataset(Dataset):\n","    def __init__(self, data, labels):\n","        self.data = data\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        return torch.tensor(self.data[idx], dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.long)\n","\n","# تابع برای بارگذاری داده‌ها به عنوان DataLoader\n","def load_data_as_dataloader(file_paths, labels, batch_size, percentage=1.0):\n","    data = []\n","    for file_path in file_paths:\n","        data.append(np.load(file_path))\n","    data = np.concatenate(data, axis=0)\n","\n","    # استفاده از درصدی از داده‌ها\n","    num_samples = data.shape[0]\n","    subset_size = int(num_samples * percentage)\n","    data = data[:subset_size]\n","    labels = labels[:subset_size]\n","\n","    dataset = CustomDataset(data, labels)\n","    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","    return dataloader\n","\n","# مدل ترنسفورمر با لایه‌های BiLSTM در PyTorch\n","class TransformerWithBiLSTM(nn.Module):\n","    def __init__(self, input_dim, num_classes, num_heads=8, num_layers=2, lstm_hidden_dim=32, dropout_rate=0.3):\n","        super(TransformerWithBiLSTM, self).__init__()\n","\n","        # لایه‌های BiLSTM\n","        self.bilstm = nn.LSTM(input_size=input_dim, hidden_size=lstm_hidden_dim,\n","                              num_layers=num_layers, batch_first=True, bidirectional=True, dropout=dropout_rate)\n","\n","        self.attention_layers = nn.ModuleList([\n","            nn.MultiheadAttention(embed_dim=lstm_hidden_dim * 2, num_heads=num_heads) for _ in range(num_layers)\n","        ])\n","\n","        self.layer_norm = nn.LayerNorm(lstm_hidden_dim * 2)\n","        self.fc1 = nn.Linear(lstm_hidden_dim * 2, 64)  # کاهش ابعاد لایه خطی\n","        self.dropout = nn.Dropout(dropout_rate)        # تغییر نرخ Dropout\n","        self.fc2 = nn.Linear(64, num_classes)          # سازگاری با ابعاد جدید\n","\n","    def forward(self, x):\n","        # عبور از لایه BiLSTM\n","        lstm_out, _ = self.bilstm(x)\n","\n","        # عبور از لایه‌های ترنسفورمر\n","        lstm_out = lstm_out.transpose(0, 1)  # برای سازگاری با PyTorch\n","        for attention in self.attention_layers:\n","            lstm_out, _ = attention(lstm_out, lstm_out, lstm_out)\n","\n","        lstm_out = lstm_out.transpose(0, 1)  # بازگشت به شکل اولیه\n","        lstm_out = self.layer_norm(lstm_out)\n","        lstm_out = torch.mean(lstm_out, dim=1)  # GlobalAveragePooling\n","        lstm_out = torch.relu(self.fc1(lstm_out))\n","        lstm_out = self.dropout(lstm_out)       # اعمال Dropout\n","        lstm_out = self.fc2(lstm_out)           # عدم نیاز به Softmax چون CrossEntropyLoss آن را خودکار انجام می‌دهد\n","        return lstm_out\n","\n","# تابع آموزش مدل\n","def train_transformer_model(train_loader, val_loader, input_dim, num_classes, model_save_path, plot_save_path, epochs=100, batch_size=32):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    # ایجاد مدل و انتقال به GPU\n","    model = TransformerWithBiLSTM(input_dim=input_dim, num_classes=num_classes)\n","    model.to(device)\n","\n","    optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=1e-4)  # استفاده از AdamW\n","    criterion = nn.CrossEntropyLoss()\n","\n","    # تنظیم نرخ یادگیری\n","    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n","\n","    # لیست‌های ذخیره تاریخچه خطا و دقت\n","    train_loss_history = []\n","    val_loss_history = []\n","\n","    best_val_accuracy = 0.0\n","    early_stopping_counter = 0  # شمارش برای Early Stopping\n","    patience = 10  # تعداد ایپوک‌ها برای توقف\n","\n","    # آموزش مدل\n","    for epoch in range(epochs):\n","        model.train()\n","        total_train_loss = 0\n","        correct_train = 0\n","        total_train = 0\n","\n","        for batch_X, batch_y in train_loader:\n","            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(batch_X)\n","            loss = criterion(outputs, batch_y)\n","            loss.backward()\n","            optimizer.step()\n","            total_train_loss += loss.item()\n","\n","            _, predicted = torch.max(outputs.data, 1)\n","            total_train += batch_y.size(0)\n","            correct_train += (predicted == batch_y).sum().item()\n","\n","        train_accuracy = 100 * correct_train / total_train\n","\n","        # اعتبارسنجی مدل\n","        model.eval()\n","        total_val_loss = 0\n","        correct_val = 0\n","        total_val = 0\n","        with torch.no_grad():\n","            for val_X, val_y in val_loader:\n","                val_X, val_y = val_X.to(device), val_y.to(device)\n","                outputs = model(val_X)\n","                loss = criterion(outputs, val_y)\n","                total_val_loss += loss.item()\n","\n","                _, predicted = torch.max(outputs.data, 1)\n","                total_val += val_y.size(0)\n","                correct_val += (predicted == val_y).sum().item()\n","\n","        val_accuracy = 100 * correct_val / total_val\n","\n","        train_loss_history.append(total_train_loss / len(train_loader))\n","        val_loss_history.append(total_val_loss / len(val_loader))\n","\n","        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {total_train_loss / len(train_loader):.4f}, Val Loss: {total_val_loss / len(val_loader):.4f}, Train Accuracy: {train_accuracy:.2f}%, Val Accuracy: {val_accuracy:.2f}%\")\n","\n","        # ذخیره مدل اگر دقت اعتبارسنجی بهبود یافته باشد\n","        if val_accuracy > best_val_accuracy:\n","            best_val_accuracy = val_accuracy\n","            torch.save(model.state_dict(), model_save_path)\n","            print(f\"New best model saved with Val Accuracy: {val_accuracy:.2f}%\")\n","            early_stopping_counter = 0  # بازنشانی شمارش\n","        else:\n","            early_stopping_counter += 1  # افزایش شمارش\n","\n","        # بروزرسانی نرخ یادگیری\n","        scheduler.step(total_val_loss)  # به روز رسانی نرخ یادگیری با توجه به ولیدیشن Loss\n","\n","        # اگر دقت ولیدیشن بهبود نیافت، آموزش متوقف شود\n","        if early_stopping_counter >= patience:\n","            print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n","            break\n","\n","    # ذخیره نمودارها\n","    plt.figure(figsize=(10, 5))\n","    plt.plot(train_loss_history, label='Train Loss')\n","    plt.plot(val_loss_history, label='Val Loss')\n","    plt.title('Model Performance')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","    plt.savefig(plot_save_path)\n","    plt.close()\n","    print(f\"Plots saved to {plot_save_path}\")\n","\n","# فراخوانی تابع train_transformer_model\n","\n","# مسیر فایل‌ها\n","save_paths_X = [\n","    '/content/drive/MyDrive/LM-RESULT2/resized_X_TRAIN_batch_1.npy',\n","    '/content/drive/MyDrive/LM-RESULT2/resized_X_TRAIN_batch_2.npy',\n","    '/content/drive/MyDrive/LM-RESULT2/resized_X_TRAIN_batch_3.npy',\n","    '/content/drive/MyDrive/LM-RESULT2/resized_X_TRAIN_batch_4.npy',\n","    '/content/drive/MyDrive/LM-RESULT2/resized_X_TRAIN_batch_5.npy',\n","    '/content/drive/MyDrive/LM-RESULT2/resized_X_TRAIN_batch_6.npy'\n","]\n","val_save_path_X = '/content/drive/MyDrive/LM-RESULT2/resized_X_val.npy'\n","val_label_csv_path = '/content/drive/MyDrive/LM-RESULT2/y_val_final.csv'\n","\n","# بارگذاری برچسب‌ها\n","train_labels = np.loadtxt('/content/drive/MyDrive/LM-RESULT2/y_train_final.csv', delimiter=',', dtype=int)\n","val_labels = np.loadtxt(val_label_csv_path, delimiter=',', dtype=int)\n","\n","# آماده‌سازی داده‌ها به عنوان DataLoader برای 100 درصد داده‌ها\n","train_loader = load_data_as_dataloader(save_paths_X, train_labels, batch_size=32, percentage=1.0)\n","val_loader = load_data_as_dataloader([val_save_path_X], val_labels, batch_size=32, percentage=1.0)\n","\n","# تنظیمات مدل و آموزش با 100 ایپوک\n","input_dim = 1024  # ابعاد ورودی داده‌ها (باید با داده‌ها سازگار باشد)\n","num_classes = 2   # تعداد کلاس‌ها (برچسب‌ها)\n","\n","train_transformer_model(\n","    train_loader,          # داده‌های آموزشی\n","    val_loader,            # داده‌های اعتبارسنجی\n","    input_dim=input_dim,   # ابعاد ورودی\n","    num_classes=num_classes,  # تعداد کلاس‌ها\n","    model_save_path='/content/drive/MyDrive/LM-RESULT2/best0021_model.pth',  # مسیر ذخیره مدل\n","    plot_save_path='/content/drive/MyDrive/LM-RESULT2/training0021_plot.png',  # مسیر ذخیره نمودار\n","    epochs=100,            # تعداد ایپوک‌ها\n","    batch_size=32          # اندازه بچ\n",")\n"],"metadata":{"id":"7ROHEE6bscpU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","from torch.utils.data import DataLoader, Dataset\n","from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, roc_auc_score, matthews_corrcoef, roc_curve\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import os\n","\n","# تعریف دیتاست سفارشی برای تست\n","class CustomTestDataset(Dataset):\n","    def __init__(self, data, labels=None):\n","        self.data = data\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        if self.labels is not None:\n","            return torch.tensor(self.data[idx], dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.long)\n","        else:\n","            return torch.tensor(self.data[idx], dtype=torch.float32)\n","\n","# تابع برای بارگذاری داده‌های تست به عنوان DataLoader\n","def load_test_data_as_dataloader(file_paths, labels=None, batch_size=32):\n","    data = []\n","    for file_path in file_paths:\n","        data.append(np.load(file_path))\n","    data = np.concatenate(data, axis=0)\n","    dataset = CustomTestDataset(data, labels)\n","    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n","    return dataloader\n","\n","# مدل ترنسفورمر با BiLSTM برای تست\n","class TransformerWithBiLSTM(nn.Module):\n","    def __init__(self, input_dim, num_classes, num_heads=8, num_layers=2, lstm_hidden_dim=32, dropout_rate=0.3):\n","        super(TransformerWithBiLSTM, self).__init__()\n","\n","        # لایه‌های BiLSTM\n","        self.bilstm = nn.LSTM(input_size=input_dim, hidden_size=lstm_hidden_dim,\n","                              num_layers=num_layers, batch_first=True, bidirectional=True, dropout=dropout_rate)\n","\n","        self.attention_layers = nn.ModuleList([nn.MultiheadAttention(embed_dim=lstm_hidden_dim * 2, num_heads=num_heads) for _ in range(num_layers)])\n","\n","        self.layer_norm = nn.LayerNorm(lstm_hidden_dim * 2)\n","        self.fc1 = nn.Linear(lstm_hidden_dim * 2, 64)  # کاهش ابعاد لایه خطی\n","        self.dropout = nn.Dropout(dropout_rate)        # تغییر نرخ Dropout\n","        self.fc2 = nn.Linear(64, num_classes)          # سازگاری با ابعاد جدید\n","\n","    def forward(self, x):\n","        # عبور از لایه BiLSTM\n","        lstm_out, _ = self.bilstm(x)\n","\n","        # عبور از لایه‌های ترنسفورمر\n","        lstm_out = lstm_out.transpose(0, 1)  # برای سازگاری با PyTorch\n","        for attention in self.attention_layers:\n","            lstm_out, _ = attention(lstm_out, lstm_out, lstm_out)\n","\n","        lstm_out = lstm_out.transpose(0, 1)  # بازگشت به شکل اولیه\n","        lstm_out = self.layer_norm(lstm_out)\n","        lstm_out = torch.mean(lstm_out, dim=1)  # GlobalAveragePooling\n","        lstm_out = torch.relu(self.fc1(lstm_out))\n","        lstm_out = self.dropout(lstm_out)       # اعمال Dropout\n","        lstm_out = torch.softmax(self.fc2(lstm_out), dim=-1)\n","        return lstm_out\n","\n","# تابع تست مدل\n","def test_transformer_model(test_loader, model_load_path, input_dim=1024, num_classes=2):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    model = TransformerWithBiLSTM(input_dim=input_dim, num_classes=num_classes)\n","    model.to(device)\n","\n","    # بارگذاری مدل ذخیره‌شده\n","    model.load_state_dict(torch.load(model_load_path, map_location=device))\n","    model.eval()\n","\n","    all_predictions = []\n","    all_labels = []\n","    all_probs = []\n","\n","    with torch.no_grad():\n","        for data, labels in test_loader:\n","            data, labels = data.to(device), labels.to(device)\n","            outputs = model(data)\n","            _, predicted = torch.max(outputs.data, 1)\n","            all_predictions.extend(predicted.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","            all_probs.extend(outputs.cpu().numpy()[:, 1])  # احتمال برچسب مثبت\n","\n","    all_probs = np.array(all_probs)\n","    return np.array(all_labels), np.array(all_predictions), all_probs\n","\n","# محاسبه و نمایش معیارهای عملکرد\n","def calculate_metrics(y_true, y_pred_labels, y_pred_probs):\n","    cm = confusion_matrix(y_true, y_pred_labels)\n","    tn, fp, fn, tp = cm.ravel()\n","    sensitivity = tp / (tp + fn) * 100\n","    specificity = tn / (tn + fp) * 100\n","    accuracy = accuracy_score(y_true, y_pred_labels) * 100\n","    f1 = f1_score(y_true, y_pred_labels) * 100\n","    mcc = matthews_corrcoef(y_true, y_pred_labels) * 100\n","    auroc = roc_auc_score(y_true, y_pred_probs) * 100\n","    print(f\"Sensitivity:     {sensitivity:.2f}%\")\n","    print(f\"Specificity:     {specificity:.2f}%\")\n","    print(f\"Accuracy:        {accuracy:.2f}%\")\n","    print(f\"F1 Score:        {f1:.2f}%\")\n","    print(f\"MCC:             {mcc:.2f}%\")\n","    print(f\"auROC:           {auroc:.2f}%\")\n","\n","# نمایش ماتریس کانفیوژن درصدی و ذخیره تصویر\n","def display_conf_matrix(y_true, y_pred_labels, title, filename):\n","    cm = confusion_matrix(y_true, y_pred_labels)\n","    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n","    plt.figure(figsize=(8, 6))\n","    sns.heatmap(cm_percent, annot=True, fmt='.2f', cmap='Blues',\n","                xticklabels=['Class 0', 'Class 1'],\n","                yticklabels=['Class 0', 'Class 1'])\n","    plt.title(title)\n","    plt.xlabel('Predicted')\n","    plt.ylabel('True')\n","    plt.savefig(filename)\n","    plt.show()\n","\n","# رسم نمودار ROC\n","def plot_roc_curve(y_true, y_pred_probs, title, filename):\n","    fpr, tpr, thresholds = roc_curve(y_true, y_pred_probs)\n","    plt.figure(figsize=(8, 6))\n","    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc_score(y_true, y_pred_probs):.2f})')\n","    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.05])\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title(title)\n","    plt.legend(loc=\"lower right\")\n","    plt.savefig(filename)\n","    plt.show()\n","\n","# مسیر فایل‌های تست\n","test_save_paths_X = ['/content/drive/MyDrive/LM-RESULT2/resized_X_TEST.npy']\n","test_label_csv_path = '/content/drive/MyDrive/LM-RESULT2/y_test_final.csv'\n","\n","# بارگذاری برچسب‌های تست\n","y_test = np.loadtxt(test_label_csv_path, delimiter=',', dtype=int)\n","\n","# آماده‌سازی داده‌ها به عنوان DataLoader برای تست\n","test_loader = load_test_data_as_dataloader(test_save_paths_X, labels=y_test, batch_size=32)\n","\n","# مسیر مدل ذخیره شده\n","model_load_path = '/content/drive/MyDrive/LM-RESULT2/best0021_model.pth'\n","\n","# تست مدل\n","y_true, y_pred_labels, predicted_probs = test_transformer_model(test_loader, model_load_path)\n","\n","# محاسبه معیارها\n","calculate_metrics(y_true, y_pred_labels, predicted_probs)\n","\n","# ایجاد دایرکتوری برای ذخیره تصاویر در صورت عدم وجود\n","output_dir = '/content/drive/MyDrive/my_project/LMPred_AMP_Prediction/Figuresfinal'\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)\n","\n","# نمایش و ذخیره ماتریس کانفیوژن\n","output_file_cm = os.path.join(output_dir, 'Transformer_BiLSTM3_Model_CM.png')\n","display_conf_matrix(y_true, y_pred_labels, 'Transformer BiLSTM Model - Confusion Matrix', output_file_cm)\n","\n","# نمایش و ذخیره نمودار ROC\n","output_file_roc = os.path.join(output_dir, 'Transformer_BiLSTM3_Model_ROC.png')\n","plot_roc_curve(y_true, predicted_probs, 'Transformer BiLSTM Model - ROC Curve', output_file_roc)\n"],"metadata":{"id":"g51O2FVKscsj"},"execution_count":null,"outputs":[]}]}